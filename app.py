import streamlit as st
import openai
import httpx
import requests
from bs4 import BeautifulSoup
from docx import Document
import PyPDF2
import pandas as pd
import os
import pythoncom
from doc2docx import convert
import re

# --- –ù–ê–°–¢–†–û–ô–ö–ò –î–û–°–¢–£–ü–ê ---
API_KEY = "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"
# –ù–æ–≤—ã–π SOCKS5 –ø—Ä–æ–∫—Å–∏
PROXY_URL = "socks5://YtvW3X:MgRYbP@45.91.209.157:12782"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π SOCKS5 –∏ –¥–æ–ª–≥–∏–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
custom_http_client = httpx.Client(
    proxy=PROXY_URL,
    timeout=httpx.Timeout(600.0, connect=60.0, read=540.0),
    trust_env=False # –ü–æ–ª–Ω—ã–π –∏–≥–Ω–æ—Ä —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
)

ai_client = openai.OpenAI(
    api_key=API_KEY,
    http_client=custom_http_client
)

st.set_page_config(page_title="AI-–¢–µ–Ω–¥–µ—Ä–Ω—ã–π –æ—Ç–¥–µ–ª v2.1", layout="wide")
st.title("ü§ñ AI-–¢–µ–Ω–¥–µ—Ä–Ω—ã–π –æ—Ç–¥–µ–ª v2.1")

if not os.path.exists("temp"):
    os.makedirs("temp")

# --- –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê ---
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_text(file_path, file_name):
    ext = file_name.split('.')[-1].lower()
    text = ""
    try:
        if ext == 'docx':
            doc = Document(file_path)
            text = "\n".join([p.text for p in doc.paragraphs])
        elif ext == 'doc':
            pythoncom.CoInitialize()
            docx_path = file_path + "x"
            convert(file_path, doc_path)
            doc = Document(docx_path)
            text = "\n".join([p.text for p in doc.paragraphs])
            if os.path.exists(docx_path): os.remove(docx_path)
        elif ext == 'pdf':
            reader = PyPDF2.PdfReader(file_path)
            text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif ext == 'xlsx':
            df = pd.read_excel(file_path)
            text = f"–¢–∞–±–ª–∏—Ü–∞ {file_name}:\n" + df.to_string()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_name}: {e}")
    return clean_text(text)

# --- –ú–û–î–£–õ–¨ –ï–ò–° (–ë–ï–ó –ü–†–û–ö–°–ò) ---
def download_eis_files(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'}
    files_data = []
    session = requests.Session()
    session.trust_env = False 
    try:
        if "common-info" in url:
            url = url.replace("common-info", "documents")
        response = session.get(url, headers=headers, timeout=20)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        download_links = [l['href'] for l in links if "download" in l['href'].lower()]
        
        if not download_links:
            st.warning("–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            return []

        progress_bar = st.progress(0)
        to_download = download_links[:5] 
        for i, link in enumerate(to_download):
            full_link = link if link.startswith('http') else "https://zakupki.gov.ru" + link
            f_resp = session.get(full_link, headers=headers)
            d_header = f_resp.headers.get('content-disposition', '')
            fname = re.findall("filename=(.+)", d_header)
            fname = fname[0].strip('"') if fname else f"doc_{i}.docx"
            fname = "".join([c for c in fname if c.isalnum() or c in "._- "]).strip()
            f_path = os.path.join("temp", fname)
            with open(f_path, "wb") as f:
                f.write(f_resp.content)
            files_data.append({"path": f_path, "name": fname})
            progress_bar.progress((i + 1) / len(to_download))
        return files_data
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ï–ò–°: {e}")
        return []

# --- –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó (–ö–ê–†–¢–ê –†–ò–°–ö–û–í) ---
def run_ai_analysis(context_text):
    if not context_text.strip():
        st.error("–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—É—Å—Ç.")
        return

    with st.spinner("–ò–ò –∑–∞–º–µ–Ω—è–µ—Ç —Ç–µ–Ω–¥–µ—Ä–Ω—ã–π –æ—Ç–¥–µ–ª: –≥–ª—É–±–æ–∫–∏–π –∞—É–¥–∏—Ç..."):
        try:
            safe_text = context_text[:120000]
            prompt = f"""
            –î–µ–π—Å—Ç–≤—É–π –∫–∞–∫ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ç–µ–Ω–¥–µ—Ä–Ω–æ–≥–æ –æ—Ç–¥–µ–ª–∞. –¢–≤–æ—è —Ü–µ–ª—å ‚Äî —Å–Ω—è—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É —Å —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è.
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:

            1. **–£–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä (–Ω–µ—Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)**: 
               - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–∞–≤–æ–∫, —Ñ–æ—Ä–º–∞—Ç —É–ø–∞–∫–æ–≤–∫–∏, –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            
            2. **–ö–∞—Ä—Ç–∞ —Ä–∏—Å–∫–æ–≤ –∏ –∑–∞–∫—Ä—ã–≤–∞—à–µ–∫**:
               - –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: –µ—Å—Ç—å –ª–∏ –±—Ä–µ–Ω–¥—ã –∏–ª–∏ "–∑–∞—Ç–æ—á–∫–∏" –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è?.
               - –°–≤–µ—Ä–∫–∞ –¢–ó —Å —Ç–∏–ø–∏—á–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏: –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–ª–∏ —Å–∫—Ä—ã—Ç—ã–µ —à—Ç—Ä–∞—Ñ—ã.
            
            3. **–¢–µ–Ω–¥–µ—Ä–Ω–∞—è –∫–æ—Ä–∑–∏–Ω–∞ (–†–∞—Å—á–µ—Ç)**:
               - –°–æ—Å—Ç–∞–≤—å —á–µ—Ç–∫–∏–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ —É –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤.
            
            4. **–ö–æ–Ω—Ç—Ä–æ–ª—å –∏ –°—Ä–æ–∫–∏**:
               - –ü—Ä–æ–≤–µ—Ä—å —Å—Ä–æ–∫–∏ –ø–æ—Å—Ç–∞–≤–∫–∏ –∏ –æ–ø–ª–∞—Ç—ã. –ù–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω–∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã?.

            –¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
            {safe_text}
            """

            response = ai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "system", "content": "–¢—ã –≤—ã—Å–æ–∫–æ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–Ω–¥–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤ –†–§."},
                          {"role": "user", "content": prompt}]
            )
            st.markdown("### üìä –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ")
            st.markdown(response.choices[0].message.content)
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ (SOCKS5): {e}")
            st.info("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –ø–∞–∫–µ—Ç httpx[socks] —á–µ—Ä–µ–∑ pip.")

# --- –ò–ù–¢–ï–†–§–ï–ô–° ---
t1, t2 = st.tabs(["üìÅ –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞", "üîó –°—Å—ã–ª–∫–∞ –ï–ò–°"])

with t1:
    up = st.file_uploader("–î–æ–∫—É–º–µ–Ω—Ç—ã (PDF, Word, Excel)", accept_multiple_files=True)
    if up and st.button("üöÄ –ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑"):
        all_c = ""
        pb = st.progress(0)
        for i, f in enumerate(up):
            p = os.path.join("temp", f.name)
            with open(p, "wb") as t: t.write(f.getbuffer())
            all_c += f"\n\n=== {f.name} ===\n" + extract_text(p, f.name)
            os.remove(p)
            pb.progress((i+1)/len(up))
        run_ai_analysis(all_c)

with t2:
    url = st.text_input("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Ç–µ–Ω–¥–µ—Ä –∏–∑ –ï–ò–°:")
    if st.button("üîç –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å"):
        files = download_eis_files(url)
        if files:
            full_t = ""
            for fi in files:
                full_t += f"\n\n=== {fi['name']} ===\n" + extract_text(fi['path'], fi['name'])
                os.remove(fi['path'])
            run_ai_analysis(full_t)